FROM python:3.11-slim

LABEL maintainer="Knowledge Center Team"
LABEL description="llama.cpp OpenAI-compatible API server"

# Install build dependencies and curl for healthcheck
RUN apt-get update && apt-get install -y \
    build-essential \
    cmake \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Create models directory
RUN mkdir -p /models

WORKDIR /app

# Install llama-cpp-python with server support
# CPU-only build by default. For GPU support, set CMAKE_ARGS before pip install
ENV CMAKE_ARGS="-DLLAMA_BLAS=ON -DLLAMA_BLAS_VENDOR=OpenBLAS"
RUN pip install --no-cache-dir llama-cpp-python[server]==0.2.90

# Environment variables for configuration
ENV MODEL_PATH=/models/model.gguf
ENV N_CTX=4096
ENV N_GPU_LAYERS=0
ENV HOST=0.0.0.0
ENV PORT=8000

# Expose the API port
EXPOSE 8000

# Health check endpoint
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
  CMD curl -f http://localhost:${PORT}/health || exit 1

# Run the llama.cpp server
CMD python -m llama_cpp.server \
    --model "${MODEL_PATH}" \
    --n_ctx "${N_CTX}" \
    --n_gpu_layers "${N_GPU_LAYERS}" \
    --host "${HOST}" \
    --port "${PORT}"
