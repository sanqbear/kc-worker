# Redis Configuration
# Redis connection URL for Celery broker and result backend
REDIS_URL=redis://localhost:6379/0

# LLM Server Configuration
# Base URL for the LLM inference server
LLM_SERVER_URL=http://localhost:8000

# LLM backend type: "llamacpp" or "vllm"
LLM_BACKEND=llamacpp

# Model name or path (backend-specific)
# For llama.cpp: path to GGUF file (e.g., "models/gguf/llama-3-Korean-Bllossom-8B-Q4_K_M.GGUF")
# For vLLM: model name (e.g., "facebook/opt-125m" or path to HF model)
LLM_MODEL=

# LLM Generation Parameters
# Maximum tokens to generate (1-32000)
MAX_TOKENS=1024

# Sampling temperature (0.0-2.0, higher = more random)
TEMPERATURE=0.7

# Nucleus sampling parameter (0.0-1.0)
TOP_P=0.95

# Top-k sampling parameter (1+)
TOP_K=40

# Task Configuration
# Soft time limit in seconds (task receives exception)
TASK_SOFT_TIME_LIMIT=180

# Hard time limit in seconds (task is killed)
TASK_TIME_LIMIT=300

# Maximum number of retries for failed tasks
TASK_MAX_RETRIES=3

# Initial retry delay in seconds (uses exponential backoff)
TASK_RETRY_DELAY=60

# Logging Configuration
# Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO

# Log format: "json" (production) or "text" (development)
LOG_FORMAT=json

# Worker Configuration
# Number of concurrent worker processes
WORKER_CONCURRENCY=4

# Number of tasks to prefetch per worker (1 = fair scheduling)
WORKER_PREFETCH_MULTIPLIER=1

# Environment
# Deployment environment: development, staging, production
ENVIRONMENT=development

# Health Check Configuration
# Enable health check HTTP server
HEALTH_CHECK_ENABLED=true

# Port for health check server
HEALTH_CHECK_PORT=8001
