FROM nvidia/cuda:12.9.1-cudnn-devel-ubuntu24.04

LABEL maintainer="Knowledge Center Team"
LABEL description="vLLM OpenAI-compatible API server"

# Set environment variables
ENV DEBIAN_FRONTEND=noninteractive
ENV PYTHONUNBUFFERED=1
ENV CUDA_HOME=/usr/local/cuda

# [중요 변경] Ubuntu 24.04에서는 pip를 통한 시스템 패키지 설치가 막혀있으므로 강제 허용
ENV PIP_BREAK_SYSTEM_PACKAGES=1

# Install system dependencies
# python3.11 -> python3 (Ubuntu 24.04 Default is 3.12)로 변경
# source build를 위해 필요한 라이브러리 추가 (libgl1 등은 vllm 의존성)
RUN apt-get update && apt-get install -y \
    python3 \
    python3-dev \
    python3-pip \
    curl \
    git \
    wget \
    vim \
    && rm -rf /var/lib/apt/lists/*

# Set python3 as default (Ubuntu 24.04는 이미 3.12가 python3로 잡혀있지만 확실히 하기 위해)
RUN ln -sf /usr/bin/python3 /usr/bin/python

# Upgrade pip
RUN python3 -m pip install --no-cache-dir --upgrade pip setuptools wheel packaging ninja

WORKDIR /app

# -----------------------------------------------------------------------------
# [RTX 5090 / sm_120 대응 설치 구간]
# vLLM 0.12.0이 PyPI에 있다면 pip install을 사용하고,
# 만약 아키텍처 호환성 문제 발생 시 아래 주석 처리된 '소스 빌드' 방식을 사용하세요.
# -----------------------------------------------------------------------------

# Case A: PIP 설치 (배포판이 5090을 지원한다고 가정 시)
RUN pip install --no-cache-dir vllm==0.12.0

# Case B: 소스 빌드 (PIP 버전이 5090을 지원하지 않거나 에러 발생 시 권장)
# RUN git clone https://github.com/vllm-project/vllm.git && \
#     cd vllm && \
#     export TORCH_CUDA_ARCH_LIST="12.0" && \
#     pip install -e .

# Create models directory
RUN mkdir -p /models

# Environment variables for configuration
ENV MODEL_NAME=/models/model
ENV TENSOR_PARALLEL_SIZE=1
ENV GPU_MEMORY_UTILIZATION=0.9
ENV MAX_MODEL_LEN=4096
ENV HOST=0.0.0.0
ENV PORT=8000

# Expose the API port
EXPOSE 8000

# Health check endpoint
HEALTHCHECK --interval=30s --timeout=10s --start-period=120s --retries=3 \
  CMD curl -f http://localhost:${PORT}/health || exit 1

# Run vLLM OpenAI-compatible server
CMD python3 -m vllm.entrypoints.openai.api_server \
    --model "${MODEL_NAME}" \
    --tensor-parallel-size "${TENSOR_PARALLEL_SIZE}" \
    --gpu-memory-utilization "${GPU_MEMORY_UTILIZATION}" \
    --max-model-len "${MAX_MODEL_LEN}" \
    --host "${HOST}" \
    --port "${PORT}"