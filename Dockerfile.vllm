FROM nvidia/cuda:12.9.1-cudnn-devel-ubuntu24.04

LABEL maintainer="Knowledge Center Team"
LABEL description="vLLM OpenAI-compatible API server"

# Set environment variables
ENV DEBIAN_FRONTEND=noninteractive
ENV PYTHONUNBUFFERED=1
ENV CUDA_HOME=/usr/local/cuda
# 시스템 패키지 강제 덮어쓰기 허용 (Ubuntu 24.04 필수)
ENV PIP_BREAK_SYSTEM_PACKAGES=1

# Install system dependencies
# python3-wheel, python3-setuptools 등은 apt로 설치되지만
# 나중에 pip로 최신 버전을 덮어씌울 것입니다.
RUN apt-get update && apt-get install -y \
    python3 \
    python3-dev \
    python3-pip \
    curl \
    git \
    wget \
    vim \
    && rm -rf /var/lib/apt/lists/*

# Set python3 as default
RUN ln -sf /usr/bin/python3 /usr/bin/python

WORKDIR /app

# -----------------------------------------------------------------------------
# [수정된 부분] pip 업그레이드 단계
# --ignore-installed: apt로 설치된 구버전 패키지를 삭제하려다 나는 에러를 방지합니다.
# -----------------------------------------------------------------------------
RUN python3 -m pip install --no-cache-dir --upgrade --ignore-installed \
    pip \
    setuptools \
    wheel \
    packaging \
    ninja

# -----------------------------------------------------------------------------
# [RTX 5090 / sm_120 대응 설치 구간]
# -----------------------------------------------------------------------------

# vLLM 설치
# (참고: 만약 pip install 실패 시, 앞서 설명드린 소스 빌드 방식을 주석 해제하여 사용하세요)
RUN pip install --no-cache-dir vllm==0.12.0

# Create models directory
RUN mkdir -p /models

# Environment variables for configuration
ENV MODEL_NAME=/models/model
ENV TENSOR_PARALLEL_SIZE=1
ENV GPU_MEMORY_UTILIZATION=0.9
ENV MAX_MODEL_LEN=4096
ENV HOST=0.0.0.0
ENV PORT=8000

# Expose the API port
EXPOSE 8000

# Health check endpoint
HEALTHCHECK --interval=30s --timeout=10s --start-period=120s --retries=3 \
  CMD curl -f http://localhost:${PORT}/health || exit 1

# Run vLLM OpenAI-compatible server
CMD python3 -m vllm.entrypoints.openai.api_server \
    --model "${MODEL_NAME}" \
    --tensor-parallel-size "${TENSOR_PARALLEL_SIZE}" \
    --gpu-memory-utilization "${GPU_MEMORY_UTILIZATION}" \
    --max-model-len "${MAX_MODEL_LEN}" \
    --host "${HOST}" \
    --port "${PORT}"